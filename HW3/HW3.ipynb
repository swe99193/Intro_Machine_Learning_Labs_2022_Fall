{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, X: np.ndarray,\n",
    "                 Y: np.ndarray,\n",
    "                 depth, criterion='gini',\n",
    "                 max_depth=None,\n",
    "                 sample_weight=None,\n",
    "                 max_features=None):\n",
    "        \"\"\"\n",
    "            X(np arr): features\n",
    "            Y(np arr): labels\n",
    "        \"\"\"\n",
    "        self.right, self.left=None, None\n",
    "        self.X, self.Y = X, Y\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.best_feature, self.best_split_value = None, None\n",
    "        self.cls = None\n",
    "        self.is_leaf = False\n",
    "        self.sample_weight = sample_weight\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def gini(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def weighted_gini(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (gini) \"\"\"\n",
    "        return (len(Y_r) * self.gini(Y_r) + len(Y_l) * self.gini(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def weighted_entropy(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (entropy) \"\"\"\n",
    "        return (len(Y_r) * self.entropy(Y_r) + len(Y_l) * self.entropy(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def sample_weight_gini(self, Y: np.ndarray, sample_weight: np.ndarray):\n",
    "        cls_list = np.unique(Y)\n",
    "\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if len(cls_list) == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        cls_count = np.zeros(len(cls_list))\n",
    "\n",
    "        for cls in cls_list:\n",
    "            cls_count[cls] = sample_weight[Y == cls].sum()\n",
    "\n",
    "        hist = cls_count / sample_weight.sum()  # normalize\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def sw_gini(self, Y_r, Y_l, sw_r, sw_l):\n",
    "        \"\"\"  sample weighted  \"\"\"\n",
    "        def gini(Y, sample_weight): return self.sample_weight_gini(\n",
    "            Y, sample_weight)\n",
    "        len_r, len_l = sw_r.sum(), sw_l.sum()\n",
    "        len_tot = len_r + len_l\n",
    "        return (len_r * gini(Y_r, sw_r) + len_l * gini(Y_l, sw_l)) / len_tot\n",
    "\n",
    "    def sample_weight_entropy(self, Y: np.ndarray, sample_weight: np.ndarray):\n",
    "        cls_list = np.unique(Y)\n",
    "\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if len(cls_list) == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        cls_count = np.zeros(len(cls_list))\n",
    "\n",
    "        for cls in cls_list:\n",
    "            cls_count[cls] = sample_weight[Y == cls].sum()\n",
    "\n",
    "        hist = cls_count / sample_weight.sum()  # normalize\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def sw_entropy(self, Y_r, Y_l, sw_r, sw_l):\n",
    "        \"\"\"  sample weighted  \"\"\"\n",
    "        def entropy(Y, sample_weight): return self.sample_weight_entropy(\n",
    "            Y, sample_weight)\n",
    "        len_r, len_l = sw_r.sum(), sw_l.sum()\n",
    "        len_tot = len_r + len_l\n",
    "        return (len_r * entropy(Y_r, sw_r) + len_l * entropy(Y_l, sw_l)) / len_tot\n",
    "\n",
    "    def sw_best_split(self):\n",
    "        \"\"\" (w/ sample weight) find the best split \"\"\"\n",
    "        X, Y, sample_weight = self.X.copy(), self.Y.copy(), self.sample_weight.copy()\n",
    "\n",
    "        min_gini = float('inf')\n",
    "\n",
    "        best_feature, best_split_value = 0, 0\n",
    "\n",
    "        for feature in range(len(X[0])):\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                mask = (X[:, feature] == 0)\n",
    "                inv_mask = (mask == False)\n",
    "                Y_l, sw_l = Y[mask], sample_weight[mask]\n",
    "                Y_r, sw_r = Y[inv_mask], sample_weight[inv_mask]\n",
    "\n",
    "                Gini = self.sw_entropy(Y_r, Y_l, sw_r, sw_l)\n",
    "                # print('feature:', feature,  'gini:',Gini)  #FIXME\n",
    "                if Gini < min_gini:\n",
    "                    min_gini = Gini\n",
    "                    best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y, sample_weight = X[idx], Y[idx], sample_weight[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    mask = (X[:, feature] <= threshold)\n",
    "                    inv_mask = (mask == False)\n",
    "                    Y_l, sw_l = Y[mask], sample_weight[mask]\n",
    "                    Y_r, sw_r = Y[inv_mask], sample_weight[inv_mask]\n",
    "\n",
    "                    Gini = self.sw_entropy(Y_r, Y_l, sw_r, sw_l)\n",
    "                    # print('feature:', feature,  'gini:',Gini)  #FIXME\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def sw_split(self):\n",
    "        \"\"\" (w/ sample weight) split this node recursively \"\"\"\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.sw_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l, sw_l = self.X[mask_l,\n",
    "                                :], self.Y[mask_l], self.sample_weight[mask_l]\n",
    "        X_r, Y_r, sw_r = self.X[mask_r,\n",
    "                                :], self.Y[mask_r], self.sample_weight[mask_r]\n",
    "\n",
    "        lnode = Node(X_l, Y_l, self.depth + 1,\n",
    "                     self.criterion, self.max_depth, sw_l)\n",
    "        rnode = Node(X_r, Y_r, self.depth + 1,\n",
    "                     self.criterion, self.max_depth, sw_r)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "\n",
    "        rnode.sw_split()\n",
    "        lnode.sw_split()\n",
    "\n",
    "    def rand_best_split(self):\n",
    "        ''' used by Random Forest '''\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        # randomly sample features\n",
    "        feature_idx = np.random.choice(\n",
    "            len(X[0]), size=self.max_features, replace=False)\n",
    "\n",
    "        best_feature, best_split_value = feature_idx[0], 0\n",
    "\n",
    "        for feature in feature_idx:\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def best_split(self):\n",
    "        \"\"\" find the best split \"\"\"\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        for feature in range(len(X[0])):\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" split this node recursively \"\"\"\n",
    "\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "        # FIXME\n",
    "        # print('depth:', self.depth)\n",
    "        # print('cls 0:', cls_count[0])\n",
    "        # print('cls 1:', cls_count[1])\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "\n",
    "            # FIXME\n",
    "            # print('predicted class:', self.cls)\n",
    "            # print()\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.best_split() if self.max_features is None \\\n",
    "            else self.rand_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # FIXME\n",
    "        # print('best_feature: ', best_feature, 'best_split_value: ', best_split_value)\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l = self.X[mask_l, :], self.Y[mask_l]\n",
    "        X_r, Y_r = self.X[mask_r, :], self.Y[mask_r]\n",
    "\n",
    "        lnode = Node(X_l, Y_l, self.depth + 1, self.criterion, self.max_depth)\n",
    "        rnode = Node(X_r, Y_r, self.depth + 1, self.criterion, self.max_depth)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "        # print()\n",
    "\n",
    "        rnode.split()\n",
    "        lnode.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion not in ['gini', 'entropy']:\n",
    "            raise Exception(\"supported criterion: 'gini' or 'entropy'\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, sample_weight=None, max_features=None):\n",
    "        \"\"\" Build a decision tree classifier from the training set (X, y).\"\"\"\n",
    "        self.X, self.Y = X.copy(), Y.copy()\n",
    "        self.root = Node(self.X,\n",
    "                         self.Y,\n",
    "                         depth=0,\n",
    "                         criterion=self.criterion,\n",
    "                         max_depth=self.max_depth,\n",
    "                         sample_weight=sample_weight,\n",
    "                         max_features=max_features)\n",
    "\n",
    "        if sample_weight is None:\n",
    "            self.root.split()\n",
    "        else:\n",
    "            self.root.sw_split()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict class value for X. \"\"\"\n",
    "        y_pred = []\n",
    "\n",
    "        for row in X_test:\n",
    "            cur_node = self.root\n",
    "\n",
    "            while not cur_node.is_leaf:\n",
    "                bf = cur_node.best_feature\n",
    "                cur_node = cur_node.left if row[bf] <= cur_node.best_split_value else cur_node.right\n",
    "\n",
    "            y_pred.append(cur_node.cls)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\" count occurrence of each best_feature \"\"\"\n",
    "        cur_node = self.root\n",
    "\n",
    "        self.feature_count = np.zeros(len(self.X[0]), dtype='uint8')\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "        return self.feature_count\n",
    "\n",
    "    def count_feature_util(self, cur_node: Node):\n",
    "        if cur_node.is_leaf:\n",
    "            return\n",
    "\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.weak_clf = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, lr=0.5):\n",
    "        self.alphas = []\n",
    "        n = self.n_estimators\n",
    "        max_depth = 1\n",
    "\n",
    "        self.Y = Y.copy()\n",
    "        # change label: 0 -> -1\n",
    "        self.Y[Y == 0] = -1\n",
    "\n",
    "        data_w = np.ones(len(Y)) / len(Y)\n",
    "\n",
    "        # train n weak classifiers\n",
    "        for i in range(n):\n",
    "            # FIXME:\n",
    "            # print(i, \"th weak clf\")\n",
    "\n",
    "            # Fit weak classifier\n",
    "            clf = DecisionTree(criterion='entropy', max_depth=max_depth)\n",
    "            # clf = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "            clf.fit(X, Y, sample_weight=data_w)\n",
    "            y_pred = clf.predict(X)\n",
    "            y_pred[y_pred == 0] = -1\n",
    "\n",
    "            # error of the clf prediction\n",
    "            error = data_w[self.Y != y_pred].sum()\n",
    "            # print(error)\n",
    "\n",
    "            # handle boundaries\n",
    "            if error >= 0.5:\n",
    "                break\n",
    "                # alpha = -10.0\n",
    "            elif error <= 0.0:\n",
    "                alpha = 10.0\n",
    "            else:\n",
    "                alpha = lr * np.log((1 - error) / error)\n",
    "\n",
    "            self.weak_clf.append(clf)\n",
    "            # alpha: weight of classifier\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            #  Update data weights\n",
    "            data_w = data_w * np.exp(- 1.0 * alpha * self.Y * y_pred)\n",
    "            data_w = data_w / data_w.sum()\n",
    "\n",
    "            # FIXME\n",
    "            # print(\"error: \", error)\n",
    "            # print(\"alpha: \", alpha)\n",
    "            # print()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        prediction = np.zeros(len(X_test))\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            # if self.alphas[i] > 0:\n",
    "            y_pred = self.weak_clf[i].predict(X_test)\n",
    "            y_pred[y_pred == 0] = -1\n",
    "            prediction += self.alphas[i] * y_pred\n",
    "\n",
    "        prediction = np.sign(prediction)\n",
    "        prediction[prediction < 0] = 0\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.60512018348798"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 0.0001\n",
    "error = 0.9999\n",
    "# error = 0.6\n",
    "1 / 2 * np.log((1 - error) / error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self,\n",
    "                 n_estimators,\n",
    "                 max_features,\n",
    "                 boostrap=True,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(max_features)\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.weak_clf = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        for i in range(self.n_estimators):\n",
    "            # bootstrapping\n",
    "            if self.boostrap:\n",
    "                sample_idx = np.random.choice(\n",
    "                    len(X), size=len(X), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arange(len(X))\n",
    "\n",
    "            clf = DecisionTree(criterion=self.criterion,\n",
    "                               max_depth=self.max_depth)\n",
    "            clf.fit(X[sample_idx], Y[sample_idx],\n",
    "                    max_features=self.max_features)\n",
    "            self.weak_clf.append(clf)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.zeros(len(X_test), dtype='int64')\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred += self.weak_clf[i].predict(X_test)\n",
    "\n",
    "        mask = (y_pred > (self.n_estimators / 2))\n",
    "        y_pred[mask] = 1\n",
    "        y_pred[(mask == False)] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Pro():\n",
    "    def __init__(self,\n",
    "                 X: np.ndarray,\n",
    "                 Y: np.ndarray,\n",
    "                 depth,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None,\n",
    "                 max_features=None,\n",
    "                 init_seed=None):\n",
    "        \"\"\"\n",
    "            X(np arr): features\n",
    "            Y(np arr): labels\n",
    "        \"\"\"\n",
    "        self.right, self.left = None, None\n",
    "        self.X, self.Y = X, Y\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.best_feature, self.best_split_value = None, None\n",
    "        self.cls = None\n",
    "        self.is_leaf = False\n",
    "        self.max_features = int(max_features)\n",
    "        self.init_seed = init_seed\n",
    "\n",
    "    def gini(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def weighted_gini(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (gini) \"\"\"\n",
    "        return (len(Y_r) * self.gini(Y_r) + len(Y_l) * self.gini(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def weighted_entropy(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (entropy) \"\"\"\n",
    "        return (len(Y_r) * self.entropy(Y_r) + len(Y_l) * self.entropy(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def rand_best_split(self):\n",
    "        ''' \n",
    "            sample a subset of features\n",
    "            used by Random forest \n",
    "        '''\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        # randomly sample features\n",
    "        np.random.seed(self.init_seed)\n",
    "        feature_idx = np.random.choice(\n",
    "            len(X[0]), size=self.max_features, replace=False)\n",
    "\n",
    "        best_feature, best_split_value = feature_idx[0], 0\n",
    "\n",
    "        for feature in feature_idx:\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini <= min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy <= min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini <= min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy <= min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" split this node recursively \"\"\"\n",
    "\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "        # FIXME\n",
    "        # print('depth:', self.depth)\n",
    "        # print('cls 0:', cls_count[0])\n",
    "        # print('cls 1:', cls_count[1])\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "\n",
    "            # FIXME\n",
    "            # print('predicted class:', self.cls)\n",
    "            # print()\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.rand_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # FIXME\n",
    "        # print('best_feature: ', best_feature, 'best_split_value: ', best_split_value)\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l = self.X[mask_l, :], self.Y[mask_l]\n",
    "        X_r, Y_r = self.X[mask_r, :], self.Y[mask_r]\n",
    "\n",
    "        lnode = Node_Pro(X_l, Y_l, self.depth + 1, self.criterion,\n",
    "                         self.max_depth, self.max_features, (2*self.init_seed) % 10000000)\n",
    "        rnode = Node_Pro(X_r, Y_r, self.depth + 1, self.criterion,\n",
    "                         self.max_depth, self.max_features, 2*self.init_seed+1 % 10000000)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "        # print()\n",
    "\n",
    "        rnode.split()\n",
    "        lnode.split()\n",
    "\n",
    "\n",
    "class DecisionTree_Pro():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion not in ['gini', 'entropy']:\n",
    "            raise Exception(\"supported criterion: 'gini' or 'entropy'\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, max_features=None, init_seed=None):\n",
    "        \"\"\" Build a decision tree classifier from the training set (X, y).\"\"\"\n",
    "        self.X, self.Y = X.copy(), Y.copy()\n",
    "        self.root = Node_Pro(self.X,\n",
    "                             self.Y,\n",
    "                             depth=0,\n",
    "                             criterion=self.criterion,\n",
    "                             max_depth=self.max_depth,\n",
    "                             max_features=max_features, init_seed=init_seed)\n",
    "\n",
    "        self.root.split()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict class value for X. \"\"\"\n",
    "        y_pred = []\n",
    "\n",
    "        for row in X_test:\n",
    "            cur_node = self.root\n",
    "\n",
    "            while not cur_node.is_leaf:\n",
    "                bf = cur_node.best_feature\n",
    "                cur_node = cur_node.left if row[bf] <= cur_node.best_split_value else cur_node.right\n",
    "\n",
    "            y_pred.append(cur_node.cls)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\" count occurrence of each best_feature \"\"\"\n",
    "        cur_node = self.root\n",
    "\n",
    "        self.feature_count = np.zeros(len(self.X[0]), dtype='uint8')\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "        return self.feature_count\n",
    "\n",
    "    def count_feature_util(self, cur_node):\n",
    "        if cur_node.is_leaf:\n",
    "            return\n",
    "\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "\n",
    "class RandomForest_Pro():\n",
    "    def __init__(self,\n",
    "                 n_estimators,\n",
    "                 max_features,\n",
    "                 boostrap=True,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.weak_clf = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, seeds):\n",
    "        for i in range(self.n_estimators):\n",
    "            # bootstrapping\n",
    "            if self.boostrap:\n",
    "                np.random.seed(seeds[i])\n",
    "                sample_idx = np.random.choice(\n",
    "                    len(X), size=len(X), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arange(len(X))\n",
    "\n",
    "            clf = DecisionTree_Pro(\n",
    "                criterion=self.criterion, max_depth=self.max_depth)\n",
    "            clf.fit(X[sample_idx], Y[sample_idx],\n",
    "                    max_features=self.max_features, init_seed=seeds[i])\n",
    "\n",
    "            y_pred = clf.predict(X)\n",
    "\n",
    "            # print('     ', accuracy_score(Y, y_pred))\n",
    "            if 0.9 < accuracy_score(Y, y_pred):\n",
    "                self.weak_clf.append(clf)\n",
    "                # print(clf.feature_importance())\n",
    "            # self.weak_clf.append(clf)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.zeros(len(X_test), dtype='int64')\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred += self.weak_clf[i].predict(X_test)\n",
    "\n",
    "        mask = (y_pred > (len(self.weak_clf) // 2))\n",
    "        y_pred[mask] = 1\n",
    "        y_pred[(mask == False)] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    if len(np.unique(sequence)) == 1:  # pure\n",
    "        return 0\n",
    "\n",
    "    hist = np.bincount(sequence) / len(sequence)\n",
    "\n",
    "    return 1 - np.sum(hist ** 2)\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    if len(np.unique(sequence)) == 1:  # pure\n",
    "        return 0\n",
    "\n",
    "    hist = np.bincount(sequence) / len(sequence)\n",
    "    hist = hist[hist != 0]  # eliminate zeros (log2)\n",
    "\n",
    "    return - np.sum(hist * log2(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n",
      "(500, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1076</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>545</td>\n",
       "      <td>1300</td>\n",
       "      <td>2043</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.3</td>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>1317</td>\n",
       "      <td>968</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1562</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>642</td>\n",
       "      <td>1533</td>\n",
       "      <td>2243</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>673</td>\n",
       "      <td>690</td>\n",
       "      <td>3438</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.8</td>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1100</td>\n",
       "      <td>1497</td>\n",
       "      <td>1665</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1076     0          2.5         0   3       0          14    0.2   \n",
       "1            759     0          2.5         0   3       1          39    0.3   \n",
       "2           1562     1          1.3         1   1       1           7    0.2   \n",
       "3           1954     0          0.6         1   8       0           7    0.9   \n",
       "4           1989     0          2.5         1   0       1          41    0.8   \n",
       "\n",
       "   mobile_wt  n_cores  pc  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        105        5   4        545      1300  2043     7     5         14   \n",
       "1        162        2   8        110      1317   968     6     2          2   \n",
       "2        190        5  15        642      1533  2243    12    10          6   \n",
       "3        126        3   9        673       690  3438    17    12         13   \n",
       "4         94        3  13       1100      1497  1665    17     9         12   \n",
       "\n",
       "   three_g  touch_screen  wifi  \n",
       "0        0             0     0  \n",
       "1        1             0     0  \n",
       "2        1             0     0  \n",
       "3        1             0     0  \n",
       "4        1             1     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "# train_df = pd.concat([train_df, val_df])\n",
    "test_df = pd.read_csv('x_test.csv')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "# train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
      "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
      "       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
      "       'touch_screen', 'wifi', 'price_range'],\n",
      "      dtype='object')\n",
      "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
      "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
      "       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
      "       'touch_screen', 'wifi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# train_df.info()\n",
    "# test_df.info()\n",
    "print(train_df.columns)\n",
    "# print(val_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(train_df.columns)\n",
    "features.remove('price_range')\n",
    "\n",
    "\n",
    "x_train = train_df[features].to_numpy()\n",
    "y_train = train_df['price_range'].to_numpy()\n",
    "x_val = val_df[features].to_numpy()\n",
    "y_val = val_df['price_range'].to_numpy()\n",
    "x_test = test_df[features].to_numpy()\n",
    "# y_test = test_df['price_range'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### column property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical: 'blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi'\n",
    "# continous: 'battery_power', 'clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "col = 'price_range'\n",
    "tmp = train_df[col].to_numpy()\n",
    "# print(tmp)\n",
    "# print(train_df[col].value_counts(sort=False))\n",
    "print(np.unique(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 3\n",
      "Test-set accuarcy score:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.fit(x_train, y_train)\n",
    "y_pred = clf_depth3.predict(x_val)\n",
    "print('Depth = 3')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 10\n",
      "Test-set accuarcy score:  0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.fit(x_train, y_train)\n",
    "y_pred = clf_depth10.predict(x_val)\n",
    "print('Depth = 10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini\n",
      "Test-set accuarcy score:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.fit(x_train, y_train)\n",
    "y_pred = clf_gini.predict(x_val)\n",
    "print('Gini')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy\n",
      "Test-set accuarcy score:  0.93\n"
     ]
    }
   ],
   "source": [
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(x_train, y_train)\n",
    "y_pred = clf_entropy.predict(x_val)\n",
    "print('Entropy')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "features = np.array(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g', 'int_memory', 'm_dep', 'mobile_wt',\n",
    "                    'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'touch_screen', 'wifi'])\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  0  0  0  2  0  0  2  1  0  0  9  5  9  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "hist = clf_depth10.feature_importance()\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 17 16 15 14 10 18 19  6  5  3  2  1  8  4  7 12 11 13  0]\n"
     ]
    }
   ],
   "source": [
    "mask_sort = hist.argsort()\n",
    "print(mask_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title('Feature Importance', {'fontsize': 20})\n",
    "ax.barh(features[mask_sort], hist[mask_sort])\n",
    "\n",
    "for i in range(10):\n",
    "    ax.axvline(x=i, linewidth=1, color='0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AdaBoost():\n",
    "#     def __init__(self, n_estimators):\n",
    "#         pass\n",
    "\n",
    "#     def fit(self, x_data, y_data):\n",
    "#         pass\n",
    "\n",
    "#     def predict(self, x_data):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10\n",
      "Test-set accuarcy score:  0.94\n"
     ]
    }
   ],
   "source": [
    "clf_Ada_10 = AdaBoost(n_estimators=10)\n",
    "clf_Ada_10.fit(x_train, y_train)\n",
    "y_pred = clf_Ada_10.predict(x_val)\n",
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Test-set accuarcy score:  0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "clf_Ada_100 = AdaBoost(n_estimators=100)\n",
    "clf_Ada_100.fit(x_train, y_train)\n",
    "y_pred = clf_Ada_100.predict(x_val)\n",
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RandomForest():\n",
    "#     def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "#         pass\n",
    "\n",
    "#     def fit(self, x_data, y_data):\n",
    "#         pass\n",
    "\n",
    "#     def predict(self, x_data):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10\n",
      "Test-set accuarcy score:  0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(\n",
    "    x_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
    "clf_10tree.fit(x_train, y_train)\n",
    "y_pred = clf_10tree.predict(x_val)\n",
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Test-set accuarcy score:  0.9633333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(\n",
    "    x_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
    "clf_100tree.fit(x_train, y_train)\n",
    "y_pred = clf_100tree.predict(x_val)\n",
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=sqrt(n)\n",
      "Test-set accuarcy score:  0.95\n"
     ]
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(\n",
    "    x_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
    "clf_random_features.fit(x_train, y_train)\n",
    "y_pred = clf_random_features.predict(x_val)\n",
    "print('max_features=sqrt(n)')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=n\n",
      "Test-set accuarcy score:  0.95\n"
     ]
    }
   ],
   "source": [
    "clf_all_features = RandomForest(\n",
    "    n_estimators=10, max_features=x_train.shape[1], boostrap=True, criterion='gini', max_depth=None)\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "y_pred = clf_all_features.predict(x_val)\n",
    "print('max_features=n')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Val-set accuarcy score:  0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "clf_Ada_100 = AdaBoost(n_estimators=100)\n",
    "clf_Ada_100.fit(x_train, y_train, lr=0.5)\n",
    "y_pred = clf_Ada_100.predict(x_val)\n",
    "print('n_estimators=100')\n",
    "print('Val-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and val data\n",
    "x_train = np.append(x_train, x_val, axis=0)\n",
    "y_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Ada_100 = AdaBoost(n_estimators=100)\n",
    "clf_Ada_100.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Val-set accuarcy score:  0.9966666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_Ada_100.predict(x_val)\n",
    "print('n_estimators=100')\n",
    "print('Val-set accuarcy score: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred: final prediction\n",
    "y_pred = clf_Ada_100.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 15\n",
    "# max_depth = 15\n",
    "# max_features = 10\n",
    "\n",
    "\n",
    "# for i in range(100):\n",
    "#     seeds = np.random.choice(1000, size=n, replace=True)\n",
    "#     print(f'seeds: {seeds}')\n",
    "\n",
    "#     clf = RandomForest_Pro(\n",
    "#         n_estimators=n, max_features=max_features, boostrap=True, max_depth=max_depth)\n",
    "#     clf.fit(x_train, y_train, seeds)\n",
    "#     y_pred = clf.predict(x_val)\n",
    "#     score = accuracy_score(y_val, y_pred)\n",
    "#     print(f'Test score={score}')\n",
    "\n",
    "#     if score > 0.975:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'y_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hsiao/GitHub/Intro_Machine_Learning_Labs_2022_Fall/HW3/HW3.ipynb Cell 62\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hsiao/GitHub/Intro_Machine_Learning_Labs_2022_Fall/HW3/HW3.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hsiao/GitHub/Intro_Machine_Learning_Labs_2022_Fall/HW3/HW3.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hsiao/GitHub/Intro_Machine_Learning_Labs_2022_Fall/HW3/HW3.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39my_test.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mprice_range\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hsiao/GitHub/Intro_Machine_Learning_Labs_2022_Fall/HW3/HW3.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest-set accuarcy score: \u001b[39m\u001b[39m'\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'y_test.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 45.0 ~ 70.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: DecisionTree(criterion='gini', max_depth=3)\n",
      "acc: 0.9397590361445783\n",
      "Testing: DecisionTree(criterion='gini', max_depth=10)\n",
      "acc: 0.9096385542168675\n",
      "Testing: DecisionTree(criterion='entropy', max_depth=3)\n",
      "acc: 0.9096385542168675\n",
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "Testing: AdaBoost(n_estimators=10)\n",
      "acc: 0.9156626506024096\n",
      "Testing: AdaBoost(n_estimators=100)\n",
      "acc: 0.9156626506024096\n",
      "Testing: RandomForest(n_estimators=10, max_features=sqrt(n_features))\n",
      "acc: 0.9337349397590361\n",
      "Testing: RandomForest(n_estimators=100, max_features=sqrt(n_features))\n",
      "acc: 0.927710843373494\n",
      "Testing: RandomForest(n_estimators=10, max_features=n_features)\n",
      "acc: 0.9397590361445783\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 45.0 ~ 70.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(f'acc: {accuracy_score(y_test, y_pred)}')\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        print(f'acc: {accuracy_score(y_test, y_pred)}')\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "print('Testing: DecisionTree(criterion=\\'gini\\', max_depth=3)')\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print('Testing: DecisionTree(criterion=\\'gini\\', max_depth=10)')\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print('Testing: DecisionTree(criterion=\\'entropy\\', max_depth=3)')\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "print('Testing: AdaBoost(n_estimators=10)')\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print('Testing: AdaBoost(n_estimators=100)')\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print('Testing: RandomForest(n_estimators=10, max_features=sqrt(n_features))')\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print('Testing: RandomForest(n_estimators=100, max_features=sqrt(n_features))')\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print('Testing: RandomForest(n_estimators=10, max_features=n_features)')\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# clf = AdaBoostClassifier(DecisionTreeClassifier(criterion = \"gini\", max_depth=1), n_estimators=10, algorithm=\"SAMME\", learning_rate=1)\n",
    "# clf = clf.fit(x_train,y_train)\n",
    "# y_pred = clf.predict(x_test)\n",
    "# print(f'acc: {accuracy_score(y_test, y_pred)}')\n",
    "# print(clf.estimator_errors_)\n",
    "# print(clf.n_classes_)\n",
    "# print(clf.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c34019f22b2cb8c0f12492c2d56cc88834c9a1831a9952841115fc570b129000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
